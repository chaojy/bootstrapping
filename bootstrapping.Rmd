---
title: "Bootstrapping"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(modelr)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```


## Simulate data
```{r}
n_samp = 250

sim_df_const =
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )

sim_df_nonconst =
  sim_df_const %>% 
  mutate(
    error = error * .75 * x,
    y = 2 + 3 * x + error
  )
```

Plot the datasets
```{r}
sim_df_const %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm")
#Equal variances. Underlying assumption for linear regression

sim_df_nonconst %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm")
#Unequal variance.  When x close to zero, residuals are very very small.  When x is far from zero, the residuals spread out.  Can still do linear regression, but one of the key assumptions - equal variance - is not met.  So difficult to make inferences - specifically difficult to construct confidence intervals.
#We will try to solve this problem using bootstrapping
```

```{r}
lm(y ~ x, data = sim_df_const) %>%  broom::tidy()
lm(y ~ x, data = sim_df_nonconst) %>%  broom::tidy()
```

Bootstrap
Try to solve this issue of unequal variances with the bootstrap
What if I had another dataset like this one - get intercept and slope
Repeat
Repeat
Could I just then look and see what is the actual variance of the intercept and slope under repeated sampling
This is what we are trying to do with the bootstrap

## Draw one bootstrap sample
First, write a bootstrap function
The reason for this is so that later, can repeat the function

```{r}
boot_sample = function(df) {
  
  sample_frac(df, replace = TRUE) %>% 
#Why bootstrap sample should be the same size as original sample?  Because confidence intervals (variance estimates) are very sensitive to sample size.  Fewer point estimate = wider confidence interval and vice versa.  Mimicking sampling from the general population
    arrange(x)
#arrange because helps to visualize the data, but not an essential step - arrange doesn't actually change your results
    
}
```

Check if this works...

```{r}
boot_sample(sim_df_nonconst) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point(alpha = .3) +
  geom_smooth(method = "lm") +
  ylim(-5, 16)
#fitted line moves slightly with each bootstrap sample

#next, can I actually estimate that intercept and slope, and save those across bootstrap samples??
```

```{r}
boot_sample(sim_df_nonconst) %>% 
  lm(y ~ x, data = .) %>% 
  broom::tidy()
#next, can I actually estimate that intercept and slope, and save those across bootstrap samples??

```

## Many samples and analysis
Generate many bootstrap samples with replace from this dataset

```{r}
boot_straps =
  tibble(
    strap_number = 1:1000,
    strap_sample = rerun(1000, boot_sample(sim_df_nonconst))
  )

#so what you have is a tibble with strap_number from 1 to 1000 and strap_sample which are the result of boot_sample bootstrap samples each consisting of a tibble for 250 rows
#check to see what is inside each tibble

boot_straps %>% pull(strap_sample)
#runs for a long time - showing all 1000 bootstrap samples
boot_straps %>% pull(strap_sample) %>% .[[1]]
boot_straps %>% pull(strap_sample) %>% .[[2]]
boot_straps %>% pull(strap_sample) %>% .[[3]]
```

Can I run my analysis on these...?
Generate models for the strap_sample tibbles

```{r}
boot_results =
  boot_straps %>% 
  mutate(
    models = map(.x = strap_sample, ~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)
  )

#Let's see what we have 
boot_results =
  boot_straps %>% 
  mutate(
    models = map(.x = strap_sample, ~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) %>% pull(results) %>% .[[1]]
#This is the intercept and slope for the first model

boot_results =
  boot_straps %>% 
  mutate(
    models = map(.x = strap_sample, ~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results)

```

What do I have now?
This is a long dataframe with 2000 rows. 1000 datasets - simple linear regression for each, and you get one intercept and slope for each.  Really for this exercise what Jeff is interested in is the estimated slope and estimated intercept.  So now let's look at distribution of slope and variance

```{r}
boot_results %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate),
    sd_est  = sd(estimate)
  )
#So, the process for bootstrap was to mimic repeated sampling.  Allowing me to get the actual variance from repeated sampling.
#When comparing the bootstrap std error of intercept and slope, to the above model (code: lm(y ~ x, data = sim_df_nonconst) %>%  broom::tidy() ), you see that the sd error of intercept from bootstrap is LESS than the std. error from off-the-shelf model.  The st error of the slope from bootstrop is higher than the std. error from off-the-shelf model
# so again, in the sm_df_nonconst dataset, the residuals are tight at intercept (x = 0) but more variance/scatter at higher values of x.  So: the bootstrap standard deviation of the intercept is LESS than in the off-the-shelf model (this is expected) whereas the bootstrap standard deviation of the slope is greater than in the off-the-shelf model (also expected). So this tells us the bootstrap is doing the right thing - the uncertainty measurements are more reflective of what is happening in this dataset
```

Look at the distributions

```{r}
boot_results %>% 
  filter(term == "x") %>% 
  ggplot(aes(x = estimate)) +
  geom_density()
#interpretation: under repeated sampling, what we are seeing is this is the actual distribution of estimated slopes is going to be.  We are not relying on orgdinary least squares or linear model theory to get an idea of what the standard error is.  We are using the bootstrap to say this is the kind of uncertainty you would see in this dataset if you did repeated sampling.
#to get confidence interval, chop off 2.5% on either tail
```

Construct bootstrap CI

```{r}
boot_results %>% 
  group_by(term) %>% 
  summarize(
    ci_lower = quantile(estimate, 0.025),
    ci_upper = quantile(estimate, 0.975)
  )
#so this is our bootstrap confidence interval
```


## Bootstrap using modelr

Can we simplify anything... ?
YES

```{r}
sim_df_nonconst %>% 
  bootstrap(1000, id = "strap_number") %>% 
#this replaces the need to write the sample above
#remodelr is creating a resampling object, not a dataframe
#in the cross validation module, we then unnested these samples into dataframes
#here, for what we want to do, we don't need to
  mutate(
    models = map(.x = strap, ~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate),
    sd_est  = sd(estimate)
  )
#very similar to above results
#this is mind-boggling that we are able to do this and the code isn't that complicated
#very powerful to do this type of an analysis this quick
#lm(y ~ x, data = sim_df_nonconst) %>%  broom::tidy()
```

Now, can we use the same code to check the constant residual dataset?

```{r}
sim_df_const %>% 
  bootstrap(1000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(y ~ x, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate),
    sd_est  = sd(estimate)
  )

lm(y ~ x, data = sim_df_const) %>%  broom::tidy()

#these estimates are very close, when the assumptions are in fact true
#bootstrapping works in both cases.
```

## Revist nyc airbnb

```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood
  ) %>% 
  filter(
    borough != "Staten Island"
  ) %>% 
  select(price, stars, borough, neighborhood, room_type)
```

```{r}
nyc_airbnb %>% 
  ggplot(aes(x = stars, y = price)) +
  geom_point()
#unequal variance at different level of stars
```

So just look at Manhattan

```{r}
nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  drop_na(stars) %>% 
  ggplot(aes(x = stars, y = price)) +
  geom_point()
#clearly nonconstant variance in the relationship between stars and price, and some outliers up there
#so repeat the example as above, use bootstrapping
#what Jeff would like to do to get the standard error for the linear model is to bootstrap - take a sample with replacement -  fit a regression of price against stars, and save the estimated intercept and slope each time.

nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  drop_na(stars) %>%
  bootstrap(1000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(price ~ stars, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate),
    sd_est  = sd(estimate)
  )
  
```

Compare this to `lm`

```{r}
nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  drop_na(stars) %>%
  lm(price ~ stars, data = .) %>% 
  broom::tidy()

#So, assuming constant variances, the standard error of lm is 4.78, which is lower than bootstrap (6.62).  So it is quite a bit higher under the bootstrap than it is under the framework that assumes constant variance.  This is what you would expect because there is hgher residual variance at higher star rating - so the slope estimate under the lm should have more variance (std error is higher)
#IF THE ACTUAL DATA SHOW MORE VARIATION, THE BOOTSTRAP VARIANCE WILL BE HIGHER THAN THE STRAIGHT UP OFF-THE-SHELF LINEAR MODEL'S VARIANCE, BECAUSE THE LINEAR MODEL ASSUMES EQUAL VARIANCES - WHEREAS THE BOOTSTRAP WILL USE ACTUAL REPEATED SAMPLING, WHICH WILL CAPTURE THE VARIABILITY IN THE ACTUAL DATA WHICH WILL TRANSLATE, OVER MANY ITERATIONS, IN THE VARIANCE OF THE BOOTSTRAP ESTIMATES OF SLOPE.
```

# IF THE ACTUAL DATA SHOW MORE VARIATION, THE BOOTSTRAP VARIANCE WILL BE HIGHER THAN THE STRAIGHT UP OFF-THE-SHELF LINEAR MODEL'S VARIANCE, BECAUSE THE LINEAR MODEL ASSUMES EQUAL VARIANCES - WHEREAS THE BOOTSTRAP WILL USE ACTUAL REPEATED SAMPLING, WHICH WILL CAPTURE THE VARIABILITY IN THE ACTUAL DATA WHICH WILL TRANSLATE, OVER MANY ITERATIONS, IN THE VARIANCE OF THE BOOTSTRAP ESTIMATES OF SLOPE.

```{r}
airbnb_boot_results =
  nyc_airbnb %>% 
  filter(borough == "Manhattan") %>% 
  drop_na(stars) %>%
  bootstrap(1000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(price ~ stars, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results) %>% 
  unnest(results)

airbnb_boot_results %>% 
  group_by(term) %>% 
  summarize(
    mean_est = mean(estimate),
    sd_est = sd(estimate)
  )

airbnb_boot_results %>% 
  filter(term == "stars") %>% 
  ggplot(aes(x = estimate)) +
  geom_density()
#interpretation: assuming constant variance, you would expect a normal distribution.  But there is some skewness - towards the LEFT.  
#here, you get a distribution of your estimates under repeated sampling without have to assuming that this follows a normal distribution - you are actually getting what this distribution looks like under repeated sampling - this is fantastic.
#Jeff doesn't do 95% CIs here but could just copy code.
```

# BOOTSTRAP DIFFERENT DATASETS - WRITE THE CODE - ONCE YOU GET USED TO MAPPING - GETTING USED TO MAPPING AND LIST COLUMNS.  BUT THIS IS PRETTY AWESOME THAT WE CAN DO BOOTSTRAPPING AND CROSS-VALIDATIONS.